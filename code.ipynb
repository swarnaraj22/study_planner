{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEpbKmDvWonZ",
        "outputId": "c473a5dc-27ff-4d93-a969-07c32eb59686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (0.8.5)\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.177.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-generativeai) (4.14.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai) (0.4.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.7.14)\n",
            "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.3\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai PyMuPDF pandas tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NCERT Class 8 Science Structured Data Extractor using Google Gemini API\n",
        "\n",
        "import os\n",
        "import json\n",
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import google.generativeai as genai\n",
        "\n",
        "# -------- CONFIGURATION --------\n",
        "API_KEY = \"Add Gemini Key\"\n",
        "PDF_PATHS = [\n",
        "    \"./hesc106.pdf\",\n",
        "    \"./hesc107.pdf\",\n",
        "\n",
        "]\n",
        "OUTPUT_DIR = \"output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# -------- PROMPT FOR STRUCTURED EXTRACTION --------\n",
        "STRUCTURED_PROMPT = \"\"\"\n",
        "You are an expert educational data extractor.\n",
        "\n",
        "Given the content of a chapter from the Class 8 NCERT Science textbook, extract the following elements in a structured format:\n",
        "\n",
        "- Chapter Name\n",
        "- Topic Name(s)\n",
        "- Sub-topic Headers\n",
        "- Paragraphs (labelled under their corresponding sub-topic)\n",
        "- Tables (if any, include captions and descriptions)\n",
        "- Figures or Images (mention their captions or figure numbers if present)\n",
        "- Examples and Activities (with proper labeling and association with sub-topics)\n",
        "- Questions and Exercises (grouped appropriately)\n",
        "- Boxed Facts or External References (label as such)\n",
        "\n",
        "Return the result in clean, structured JSON with this hierarchy:\n",
        "\n",
        "{\n",
        "  \"chapter_name\": \"\",\n",
        "  \"topics\": [\n",
        "    {\n",
        "      \"topic_name\": \"\",\n",
        "      \"subtopics\": [\n",
        "        {\n",
        "          \"subtopic_name\": \"\",\n",
        "          \"content\": {\n",
        "            \"paragraphs\": [],\n",
        "            \"tables\": [],\n",
        "            \"figures\": [],\n",
        "            \"examples\": [],\n",
        "            \"activities\": [],\n",
        "            \"questions\": [],\n",
        "            \"boxed_facts\": []\n",
        "          }\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "Do not interpret or summarize. Only extract exactly what appears in the chapter.\n",
        "\"\"\"\n",
        "\n",
        "# -------- LOAD CHAPTER TEXT FROM PDF --------\n",
        "def extract_text_from_pdf(path):\n",
        "    doc = fitz.open(path)\n",
        "    full_text = \"\"\n",
        "    for page in doc:\n",
        "        full_text += page.get_text()\n",
        "    return full_text\n",
        "\n",
        "# -------- CALL GOOGLE GEMINI API --------\n",
        "def extract_structured_json(text):\n",
        "    import re\n",
        "    genai.configure(api_key=API_KEY)\n",
        "    model = genai.GenerativeModel(\"models/gemini-1.5-flash-latest\")\n",
        "\n",
        "    try:\n",
        "        print(f\"📄 Sending input with {len(text)} characters...\")\n",
        "        response = model.generate_content([STRUCTURED_PROMPT, text])\n",
        "        print(\"✅ Raw response received\")\n",
        "\n",
        "        content = response.text.strip()\n",
        "        print(\"🔍 First 300 characters of response:\\n\" + content[:300])\n",
        "\n",
        "        if content.startswith(\"```json\"):\n",
        "            content = content.replace(\"```json\", \"\").strip()\n",
        "        if content.endswith(\"```\"):\n",
        "            content = content[:-3].strip()\n",
        "\n",
        "        try:\n",
        "            return json.loads(content)\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"⚠️ Attempting to repair malformed JSON...\")\n",
        "            json_match = re.search(r'\\{[\\s\\S]*\\}', content)\n",
        "            if json_match:\n",
        "                repaired = json_match.group()\n",
        "                return json.loads(repaired)\n",
        "            else:\n",
        "                raise\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"❌ Response was not valid JSON even after repair. Saving raw response.\")\n",
        "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "        with open(os.path.join(OUTPUT_DIR, \"last_raw_response.txt\"), \"w\") as f:\n",
        "            f.write(response.text)\n",
        "        return {}\n",
        "    except Exception as e:\n",
        "        print(\"💥 API Error:\", e)\n",
        "        return {}\n",
        "\n",
        "\n",
        "# -------- SAVE OUTPUT --------\n",
        "def save_json(obj, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(obj, f, indent=2)\n",
        "\n",
        "def json_to_excel(json_data, excel_path):\n",
        "    rows = []\n",
        "    for topic in json_data.get('topics', []):\n",
        "        topic_name = topic.get('topic_name', '')\n",
        "        for sub in topic.get('subtopics', []):\n",
        "            subtopic_name = sub.get('subtopic_name', '')\n",
        "            content = sub.get('content', {})\n",
        "\n",
        "            def safe_join(items):\n",
        "                if not isinstance(items, list):\n",
        "                    return \"\"\n",
        "                return \" | \".join(str(i) if isinstance(i, str) else json.dumps(i) for i in items)\n",
        "\n",
        "            row = {\n",
        "                'Topic': topic_name,\n",
        "                'Sub-topic': subtopic_name,\n",
        "                'Paragraphs': safe_join(content.get('paragraphs', [])),\n",
        "                'Tables': safe_join(content.get('tables', [])),\n",
        "                'Figures': safe_join(content.get('figures', [])),\n",
        "                'Examples': safe_join(content.get('examples', [])),\n",
        "                'Activities': safe_join(content.get('activities', [])),\n",
        "                'Questions': safe_join(content.get('questions', [])),\n",
        "                'Boxed Facts': safe_join(content.get('boxed_facts', []))\n",
        "            }\n",
        "            rows.append(row)\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_excel(excel_path, index=False)\n",
        "\n",
        "# -------- RUN EXTRACTION --------\n",
        "all_json_data = []\n",
        "for path in tqdm(PDF_PATHS):\n",
        "    chapter_text = extract_text_from_pdf(path)\n",
        "    structured = extract_structured_json(chapter_text)\n",
        "\n",
        "    # Save\n",
        "    base_name = os.path.basename(path).replace(\".pdf\", \"\")\n",
        "    json_path = os.path.join(OUTPUT_DIR, f\"{base_name}.json\")\n",
        "    save_json(structured, json_path)\n",
        "\n",
        "    if isinstance(structured, dict) and structured:\n",
        "        excel_path = os.path.join(OUTPUT_DIR, f\"{base_name}.xlsx\")\n",
        "        json_to_excel(structured, excel_path)\n",
        "        all_json_data.append(structured)\n",
        "\n",
        "# -------- SAVE FINAL CONSOLIDATED JSON --------\n",
        "final_json_path = os.path.join(OUTPUT_DIR, \"chapter-extract.json\")\n",
        "with open(final_json_path, \"w\") as f:\n",
        "    json.dump(all_json_data, f, indent=2)\n",
        "\n",
        "# -------- STUDY PLANNER GENERATOR --------\n",
        "def generate_study_plan(json_data, total_days):\n",
        "    topics = []\n",
        "    for chapter in json_data:\n",
        "        chapter_name = chapter.get(\"chapter_name\", \"\")\n",
        "        for topic in chapter.get(\"topics\", []):\n",
        "            topic_name = topic.get(\"topic_name\", \"\")\n",
        "            topic_count = sum(len(sub.get(\"content\", {}).get(\"paragraphs\", [])) for sub in topic.get(\"subtopics\", []))\n",
        "            topics.append((chapter_name, topic_name, topic_count))\n",
        "\n",
        "    total_weight = sum(weight for _, _, weight in topics)\n",
        "    days_allocated = []\n",
        "    for chapter, topic, weight in topics:\n",
        "        days = max(1, round((weight / total_weight) * total_days))\n",
        "        days_allocated.append((chapter, topic, days))\n",
        "\n",
        "    # Flatten into day-wise schedule\n",
        "    schedule = []\n",
        "    day = 1\n",
        "    for chapter, topic, days in days_allocated:\n",
        "        for d in range(days):\n",
        "            schedule.append({\n",
        "                \"Day\": day,\n",
        "                \"Chapter\": chapter,\n",
        "                \"Topic\": topic\n",
        "            })\n",
        "            day += 1\n",
        "\n",
        "    planner_path = os.path.join(OUTPUT_DIR, \"study_planner_part1.xlsx\")\n",
        "    pd.DataFrame(schedule).to_excel(planner_path, index=False)\n",
        "    print(f\"Study planner saved to {planner_path}\")\n",
        "\n",
        "# -------- INVOKE STUDY PLANNER --------\n",
        "try:\n",
        "    days = int(input(\"Enter total number of days for the study planner (e.g., 10): \"))\n",
        "    with open(final_json_path) as f:\n",
        "        consolidated_data = json.load(f)\n",
        "    generate_study_plan(consolidated_data, days)\n",
        "except Exception as e:\n",
        "    print(\"Error generating study planner:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "JedHbgKId1Nj",
        "outputId": "75be905f-e6ea-475d-f3cc-e81466107e21"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Sending input with 21620 characters...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 1/2 [00:46<00:46, 46.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Raw response received\n",
            "🔍 First 300 characters of response:\n",
            "```json\n",
            "{\n",
            "  \"chapter_name\": \"Reproduction in Animals\",\n",
            "  \"topics\": [\n",
            "    {\n",
            "      \"topic_name\": \"Modes of Reproduction\",\n",
            "      \"subtopics\": [\n",
            "        {\n",
            "          \"subtopic_name\": \"Modes of Reproduction\",\n",
            "          \"content\": {\n",
            "            \"paragraphs\": [\n",
            "              \"Have you seen the young ones of\n",
            "📄 Sending input with 25772 characters...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [01:35<00:00, 47.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Raw response received\n",
            "🔍 First 300 characters of response:\n",
            "```json\n",
            "{\n",
            "  \"chapter_name\": \"Reaching the Age of Adolescence\",\n",
            "  \"topics\": [\n",
            "    {\n",
            "      \"topic_name\": \"Adolescence and Puberty\",\n",
            "      \"subtopics\": [\n",
            "        {\n",
            "          \"subtopic_name\": \"Adolescence and Puberty\",\n",
            "          \"content\": {\n",
            "            \"paragraphs\": [\n",
            "              \"Boojho was celebrat\n",
            "Enter total number of days for the study planner (e.g., 10): 80\n",
            "Study planner saved to output/study_planner_part1.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import google.generativeai as genai\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# -------- CONFIGURATION --------\n",
        "API_KEY = \"AIzaSyCVH7_QDZaEudYjp9Vioj1N6bEfPDFu_JY\"  # Replace with your actual API key\n",
        "PDF_PATHS = [\n",
        "\n",
        "    \"./hesc108.pdf\",\n",
        "    \"./hesc113.pdf\"\n",
        "]\n",
        "OUTPUT_DIR = \"output_part2\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# -------- ENHANCED PROMPT --------\n",
        "STRUCTURED_PROMPT = \"\"\"\n",
        "You are an expert educational content parser. Extract NCERT Class 8 Science content in this EXACT JSON format:\n",
        "\n",
        "{\n",
        "  \"chapter_name\": \"string\",\n",
        "  \"topics\": [\n",
        "    {\n",
        "      \"topic_name\": \"string\",\n",
        "      \"subtopics\": [\n",
        "        {\n",
        "          \"subtopic_name\": \"string\",\n",
        "          \"content\": {\n",
        "            \"paragraphs\": [\"string\"],\n",
        "            \"tables\": [{\"caption\": \"string\", \"content\": \"string\"}],\n",
        "            \"figures\": [{\"caption\": \"string\", \"description\": \"string\"}],\n",
        "            \"examples\": [\"string\"],\n",
        "            \"activities\": [\"string\"],\n",
        "            \"questions\": [\"string\"],\n",
        "            \"boxed_facts\": [\"string\"]\n",
        "          }\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "STRICT RULES:\n",
        "1. Only return pure JSON with no surrounding text\n",
        "2. Escape all special characters in strings\n",
        "3. No trailing commas\n",
        "4. All brackets must be balanced\n",
        "5. Use empty arrays for missing data\n",
        "6. Maintain exactly this structure\n",
        "7. Preserve original text without interpretation\n",
        "\"\"\"\n",
        "\n",
        "# -------- PDF PROCESSING --------\n",
        "def extract_text_from_pdf(path, max_pages=None):\n",
        "    \"\"\"Extract text from PDF with page limit for large files\"\"\"\n",
        "    doc = fitz.open(path)\n",
        "    full_text = \"\"\n",
        "    for i, page in enumerate(doc):\n",
        "        if max_pages and i >= max_pages:\n",
        "            break\n",
        "        full_text += page.get_text()\n",
        "    return full_text\n",
        "\n",
        "# -------- JSON VALIDATION & REPAIR --------\n",
        "def repair_json(json_str):\n",
        "    \"\"\"Comprehensive JSON repair with multiple strategies\"\"\"\n",
        "    try:\n",
        "        # Remove common non-JSON artifacts\n",
        "        json_str = re.sub(r'^[^{]*', '', json_str)  # Remove text before first {\n",
        "        json_str = re.sub(r'[^}]*$', '', json_str)  # Remove text after last }\n",
        "\n",
        "        # Fix common JSON issues\n",
        "        json_str = re.sub(r',\\s*([}\\]])', r'\\1', json_str)  # Remove trailing commas\n",
        "        json_str = re.sub(r'\\\\\\'', \"'\", json_str)  # Fix escaped quotes\n",
        "        json_str = re.sub(r'([{\\[,])\\s*([}\\]])', r'\\1\\2', json_str)  # Fix empty elements\n",
        "\n",
        "        # Balance brackets if needed\n",
        "        open_braces = json_str.count('{')\n",
        "        close_braces = json_str.count('}')\n",
        "        if open_braces > close_braces:\n",
        "            json_str += '}' * (open_braces - close_braces)\n",
        "        elif close_braces > open_braces:\n",
        "            json_str = '{' * (close_braces - open_braces) + json_str\n",
        "\n",
        "        return json_str.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Repair error: {str(e)}\")\n",
        "        return json_str\n",
        "\n",
        "def validate_json_structure(data):\n",
        "    \"\"\"Thorough validation of extracted structure\"\"\"\n",
        "    if not isinstance(data, dict):\n",
        "        return False\n",
        "    if 'chapter_name' not in data or not isinstance(data['chapter_name'], str):\n",
        "        return False\n",
        "    if 'topics' not in data or not isinstance(data['topics'], list):\n",
        "        return False\n",
        "\n",
        "    for topic in data['topics']:\n",
        "        if not isinstance(topic, dict) or 'topic_name' not in topic:\n",
        "            return False\n",
        "        if 'subtopics' not in topic or not isinstance(topic['subtopics'], list):\n",
        "            return False\n",
        "\n",
        "        for subtopic in topic['subtopics']:\n",
        "            if not isinstance(subtopic, dict) or 'subtopic_name' not in subtopic:\n",
        "                return False\n",
        "            if 'content' not in subtopic or not isinstance(subtopic['content'], dict):\n",
        "                return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# -------- API COMMUNICATION --------\n",
        "def process_with_gemini(text, max_retries=3):\n",
        "    \"\"\"Handle API communication with retries\"\"\"\n",
        "    genai.configure(api_key=API_KEY)\n",
        "    model = genai.GenerativeModel(\"models/gemini-1.5-flash-latest\")\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content([STRUCTURED_PROMPT, text])\n",
        "            content = response.text.strip()\n",
        "\n",
        "            # Initial parse attempt\n",
        "            try:\n",
        "                data = json.loads(content)\n",
        "                if validate_json_structure(data):\n",
        "                    return data\n",
        "            except json.JSONDecodeError:\n",
        "                pass\n",
        "\n",
        "            # Repair attempt\n",
        "            repaired = repair_json(content)\n",
        "            try:\n",
        "                data = json.loads(repaired)\n",
        "                if validate_json_structure(data):\n",
        "                    return data\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return None\n",
        "\n",
        "# -------- DATA PROCESSING --------\n",
        "def process_pdf(path):\n",
        "    \"\"\"Process a single PDF file\"\"\"\n",
        "    try:\n",
        "        print(f\"\\nProcessing {os.path.basename(path)}...\")\n",
        "        text = extract_text_from_pdf(path, max_pages=20)  # Limit pages for large files\n",
        "\n",
        "        if not text:\n",
        "            print(\"⚠️ No text extracted\")\n",
        "            return None\n",
        "\n",
        "        # Process in chunks if needed\n",
        "        max_chars = 30000\n",
        "        if len(text) > max_chars:\n",
        "            print(f\"Trimming from {len(text)} to {max_chars} chars\")\n",
        "            text = text[:max_chars]\n",
        "\n",
        "        data = process_with_gemini(text)\n",
        "        if not data:\n",
        "            print(\"❌ Failed to extract structured data\")\n",
        "            return None\n",
        "\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"💥 Processing error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# -------- OUTPUT GENERATION --------\n",
        "def save_results(data, base_name):\n",
        "    \"\"\"Save results in multiple formats\"\"\"\n",
        "    if not data:\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Save JSON\n",
        "        json_path = os.path.join(OUTPUT_DIR, f\"{base_name}.json\")\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump(data, f, indent=2)\n",
        "        print(f\"💾 Saved JSON to {json_path}\")\n",
        "\n",
        "        # Save Excel\n",
        "        excel_path = os.path.join(OUTPUT_DIR, f\"{base_name}.xlsx\")\n",
        "        rows = []\n",
        "\n",
        "        for topic in data.get('topics', []):\n",
        "            for subtopic in topic.get('subtopics', []):\n",
        "                content = subtopic.get('content', {})\n",
        "                row = {\n",
        "                    'Chapter': data.get('chapter_name', ''),\n",
        "                    'Topic': topic.get('topic_name', ''),\n",
        "                    'Subtopic': subtopic.get('subtopic_name', ''),\n",
        "                    'Paragraphs': '\\n'.join(content.get('paragraphs', [])),\n",
        "                    'Tables': len(content.get('tables', [])),\n",
        "                    'Figures': len(content.get('figures', [])),\n",
        "                    'Examples': len(content.get('examples', [])),\n",
        "                    'Activities': len(content.get('activities', [])),\n",
        "                    'Questions': len(content.get('questions', [])),\n",
        "                    'Boxed Facts': len(content.get('boxed_facts', []))\n",
        "                }\n",
        "                rows.append(row)\n",
        "\n",
        "        pd.DataFrame(rows).to_excel(excel_path, index=False)\n",
        "        print(f\"💾 Saved Excel to {excel_path}\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Output saving failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# -------- MAIN EXECUTION --------\n",
        "def main():\n",
        "    all_data = []\n",
        "\n",
        "    # Process files in parallel\n",
        "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "        futures = {executor.submit(process_pdf, path): path for path in PDF_PATHS}\n",
        "\n",
        "        for future in tqdm(as_completed(futures), total=len(PDF_PATHS)):\n",
        "            path = futures[future]\n",
        "            data = future.result()\n",
        "\n",
        "            if data:\n",
        "                base_name = os.path.basename(path).replace('.pdf', '')\n",
        "                if save_results(data, base_name):\n",
        "                    all_data.append(data)\n",
        "\n",
        "    # Save consolidated results\n",
        "    if all_data:\n",
        "        consolidated_path = os.path.join(OUTPUT_DIR, \"consolidated.json\")\n",
        "        with open(consolidated_path, 'w') as f:\n",
        "            json.dump(all_data, f, indent=2)\n",
        "        print(f\"\\n✅ Consolidated data saved to {consolidated_path}\")\n",
        "\n",
        "        # Generate study plan\n",
        "        generate_study_plan(all_data)\n",
        "    else:\n",
        "        print(\"\\n❌ No successful extractions\")\n",
        "\n",
        "# -------- STUDY PLAN GENERATOR --------\n",
        "def generate_study_plan(data, default_days=30):\n",
        "    if not data:\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        days = int(input(f\"\\nEnter study days (default {default_days}): \") or default_days)\n",
        "    except:\n",
        "        days = default_days\n",
        "\n",
        "    plan = []\n",
        "    day_counter = 1\n",
        "\n",
        "    for chapter in data:\n",
        "        chapter_name = chapter.get('chapter_name', 'Unknown')\n",
        "        topics = chapter.get('topics', [])\n",
        "\n",
        "        for topic in topics:\n",
        "            topic_name = topic.get('topic_name', 'Unknown')\n",
        "            subtopics = topic.get('subtopics', [])\n",
        "\n",
        "            # Calculate weight based on content\n",
        "            weight = sum(\n",
        "                len(subtopic.get('content', {}).get('paragraphs', []))\n",
        "                for subtopic in subtopics\n",
        "            )\n",
        "\n",
        "            # Allocate days proportionally\n",
        "            allocated_days = max(1, round(weight / 10))  # 1 day per ~10 paragraphs\n",
        "            plan.append({\n",
        "                'Day Range': f\"{day_counter}-{day_counter + allocated_days - 1}\",\n",
        "                'Days': allocated_days,\n",
        "                'Chapter': chapter_name,\n",
        "                'Topic': topic_name,\n",
        "                'Subtopics': ', '.join(s.get('subtopic_name', '') for s in subtopics)\n",
        "            })\n",
        "            day_counter += allocated_days\n",
        "\n",
        "    # Save plan\n",
        "    plan_path = os.path.join(OUTPUT_DIR, \"study_plan_part2.xlsx\")\n",
        "    pd.DataFrame(plan).to_excel(plan_path, index=False)\n",
        "    print(f\"📅 Study plan saved to {plan_path}\")\n",
        "    print(f\"\\nTotal study days: {day_counter - 1}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "4w98HEe0ocpw",
        "outputId": "72bf108a-ce42-4a35-c3ef-ae88e06d7d9a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing hesc108.pdf...\n",
            "\n",
            "Processing hesc113.pdf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trimming from 33161 to 30000 chars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 1/2 [00:51<00:51, 51.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempt 1 failed: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "💾 Saved JSON to output_part2/hesc108.json\n",
            "💾 Saved Excel to output_part2/hesc108.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [01:34<00:00, 47.28s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saved JSON to output_part2/hesc113.json\n",
            "💾 Saved Excel to output_part2/hesc113.xlsx\n",
            "\n",
            "✅ Consolidated data saved to output_part2/consolidated.json\n",
            "\n",
            "Enter study days (default 30): 80\n",
            "📅 Study plan saved to output_part2/study_plan_part2.xlsx\n",
            "\n",
            "Total study days: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zwT21G9TajDI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
